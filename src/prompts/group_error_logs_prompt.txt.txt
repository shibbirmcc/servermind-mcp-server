You are given Splunk ERROR log events in JSON format.

Goal
Group events by their underlying error pattern (semantics), not exact text, and for each group select ONE representative trace/correlation ID to follow up with in the next step.

Step 0 — Input shape
- Input is a JSON array of events (objects) from Splunk.
- Preserve events exactly as received (no field renaming/alteration).

Step 1 — Find the linking ID field (trace/correlation)
1) Preferred field names (case/underscore-insensitive):
   traceId, trace_id, traceID, correlationId, correlation_id, requestId, request_id, x-b3-traceid, b3TraceId
2) If none match, scan all fields for values matching ANY of:
   - UUID: 8-4-4-4-12 hex (case-insensitive)
   - Long hex string: ≥16 hex chars
   - Alphanumeric token: ≥12 chars, no spaces
3) Select the BEST candidate field by:
   - Highest presence across events (coverage %)
   - Tie-breaker: highest number of unique values
4) If no suitable field is found in ANY event:
   OUTPUT ONLY this plain text (no JSON):
   "No suitable trace/correlation ID found in logs. Cannot continue grouping step."

Step 2 — Build a normalized message for semantic grouping
For each event, derive a normalized message string from the best available text field(s) (e.g., message, msg, error, exception, _raw):
- Lowercase it.
- Strip timestamps and ISO datetimes.
- Replace IDs (UUIDs, long hex, 12+ alnum) with the token "<ID>".
- Replace numbers (ints/floats), emails, IP addresses, and quoted tokens with "<VAL>".
- Collapse whitespace and punctuation runs.
This yields a message "template" that generalizes variable parts, e.g.:
  "failed to update person <VAL>"  (covers "failed to update person ABC/BCD/...")

Step 3 — Semantic clustering
- Primary key: the normalized message template.
- Secondary: cosine/semantic similarity between templates to merge near-duplicates.
  (If two templates differ only by minor tokens and clearly describe the same failure, merge them.)
- Keep clusters tight; avoid over-merging dissimilar failures.
- Maintain original event order within each resulting cluster.

Step 4 — Choose a representative ID per cluster
- For the chosen ID field from Step 1, collect IDs appearing in the cluster.
- Pick ONE representative ID per cluster using:
  1) Most frequent ID within the cluster.
  2) Tie-breaker: the ID whose events span the richest cross-service coverage (more distinct services/sources).
  3) Final tie-breaker: the most recent event timestamp.
- If a cluster has zero events with the ID field (all missing), set "chosen_id" to null.

Step 5 — Output
Output ONLY valid JSON (unless instructed above to return a single-line plain text error). The JSON must be an array of groups:
[
  {
    "pattern": "<human-readable short label for the error pattern>",
    "template": "<normalized message template>",
    "count": <number_of_events_in_group>,
    "chosen_id": "<the selected trace/correlation ID or null>",
    "all_ids": ["<id1>", "<id2>", ...],           // unique IDs in this group (omit if none)
    "sample_events": [ { ... }, { ... } ]          // up to 5 representative events (preserve fields)
  },
  ...
]

Additional rules
- If some events lack the chosen ID field, EXCLUDE them from "all_ids" consideration but they may still be part of the group’s events.
- If ANY events were excluded from grouping because they lacked a usable text field to normalize, prepend this single plain-text line BEFORE the JSON:
  "⚠️ Some events were skipped due to missing message content."
- Keep JSON strictly valid; do not include commentary after the JSON.
- Aim for at most 8–12 groups by merging near-duplicates; prefer fewer, clearer groups over many tiny ones.
